{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54ccdab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.294915"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "a = 0.4\n",
    "b = 0.2\n",
    "float(requests.get(f\"http://ramcdougal.com/cgi-bin/error_function.py?a={a}&b={b}\", headers={\"User-Agent\": \"MyScript\"}).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be4afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "BASE_URL = \"http://ramcdougal.com/cgi-bin/error_function.py\"\n",
    "HEADERS = {\"User-Agent\": \"MyScript\"}  \n",
    "\n",
    "def get_error(a, b):\n",
    "    resp = requests.get(\n",
    "        BASE_URL,\n",
    "        params={\"a\": a, \"b\": b},\n",
    "        headers=HEADERS\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "    return float(resp.text.strip())\n",
    "\n",
    "def estimate_gradient(a, b, h=1e-3):\n",
    "    # partial wrt a\n",
    "    if a - h >= 0 and a + h <= 1:\n",
    "        Ea_plus  = get_error(a + h, b)\n",
    "        Ea_minus = get_error(a - h, b)\n",
    "        dEa = (Ea_plus - Ea_minus) / (2 * h)\n",
    "    elif a - h < 0:  # near lower bound\n",
    "        Ea_plus  = get_error(a + h, b)\n",
    "        Ea       = get_error(a, b)\n",
    "        dEa = (Ea_plus - Ea) / h\n",
    "    else:            # near upper bound\n",
    "        Ea       = get_error(a, b)\n",
    "        Ea_minus = get_error(a - h, b)\n",
    "        dEa = (Ea - Ea_minus) / h\n",
    "\n",
    "    # partial wrt b\n",
    "    if b - h >= 0 and b + h <= 1:\n",
    "        Eb_plus  = get_error(a, b + h)\n",
    "        Eb_minus = get_error(a, b - h)\n",
    "        dEb = (Eb_plus - Eb_minus) / (2 * h)\n",
    "    elif b - h < 0:\n",
    "        Eb_plus = get_error(a, b + h)\n",
    "        Eb      = get_error(a, b)\n",
    "        dEb = (Eb_plus - Eb) / h\n",
    "    else:\n",
    "        Eb      = get_error(a, b)\n",
    "        Eb_minus = get_error(a, b - h)\n",
    "        dEb = (Eb - Eb_minus) / h\n",
    "\n",
    "    return dEa, dEb\n",
    "\n",
    "def gradient_descent_2d(\n",
    "    a0,\n",
    "    b0,\n",
    "    alpha=0.1,\n",
    "    h=1e-3,\n",
    "    tol_grad=1e-4,\n",
    "    tol_param=1e-5,\n",
    "    max_iters=200\n",
    "):\n",
    "    a, b = a0, b0\n",
    "    E = get_error(a, b)\n",
    "\n",
    "    for it in range(max_iters):\n",
    "        dEa, dEb = estimate_gradient(a, b, h=h)\n",
    "\n",
    "        # gradient norm as a stopping measure\n",
    "        grad_norm = math.sqrt(dEa**2 + dEb**2)\n",
    "        if grad_norm < tol_grad:\n",
    "            break\n",
    "\n",
    "        # gradient descent step\n",
    "        new_a = a - alpha * dEa\n",
    "        new_b = b - alpha * dEb\n",
    "\n",
    "        # project back into [0, 1]\n",
    "        new_a = min(1.0, max(0.0, new_a))\n",
    "        new_b = min(1.0, max(0.0, new_b))\n",
    "\n",
    "        # check parameter change\n",
    "        delta = math.sqrt((new_a - a)**2 + (new_b - b)**2)\n",
    "        a, b = new_a, new_b\n",
    "        new_E = get_error(a, b)\n",
    "\n",
    "        if delta < tol_param:\n",
    "            E = new_E\n",
    "            break\n",
    "\n",
    "        E = new_E\n",
    "\n",
    "    return a, b, E, it + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08b4c782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7119987415000189 0.1690137364999897 1.00000000057 11\n"
     ]
    }
   ],
   "source": [
    "a_opt, b_opt, err_opt, n = gradient_descent_2d(0.7, 0.3)\n",
    "print(a_opt, b_opt, err_opt, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e6eacae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start=(0.100,0.100) -> a=0.215992, b=0.688960, err=1.100000, iters=44\n",
      "Start=(0.900,0.900) -> a=0.216047, b=0.689014, err=1.100000, iters=44\n",
      "Start=(0.100,0.900) -> a=0.215981, b=0.689035, err=1.100000, iters=40\n",
      "Start=(0.900,0.100) -> a=0.712008, b=0.168997, err=1.000000, iters=12\n",
      "Start=(0.500,0.500) -> a=0.216038, b=0.688975, err=1.100000, iters=41\n",
      "Start=(0.852,0.160) -> a=0.712015, b=0.168999, err=1.000000, iters=11\n"
     ]
    }
   ],
   "source": [
    "starts = [\n",
    "    (0.1, 0.1),\n",
    "    (0.9, 0.9),\n",
    "    (0.1, 0.9),\n",
    "    (0.9, 0.1),\n",
    "    (0.5, 0.5),\n",
    "    (random.random(), random.random())\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for (a0, b0) in starts:\n",
    "    a_opt, b_opt, err_opt, n = gradient_descent_2d(a0, b0)\n",
    "    results.append((a0, b0, a_opt, b_opt, err_opt, n))\n",
    "\n",
    "for r in results:\n",
    "    print(f\"Start=({r[0]:.3f},{r[1]:.3f}) -> \"\n",
    "          f\"a={r[2]:.6f}, b={r[3]:.6f}, err={r[4]:.6f}, iters={r[5]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
